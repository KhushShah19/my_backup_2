{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input -> Exponentiate -> Normalize -> Output\n",
    "# Softmax Function == Exponentiate + Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.718281828459045\n"
     ]
    }
   ],
   "source": [
    "import math # just for understanding\n",
    "exponential_math = math.e\n",
    "print(exponential_math) # e == 2.718281828459045"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = [[3.1, 4.3, -2.4],\n",
    "                 [-2, -1, 4.5], # another way to deal with negatives\n",
    "                 [-1.8, -0.9, 0.1]] \n",
    "simple_layer = [-1.8, -0.9, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.16529888822158656, 0.4065696597405991, 1.1051709180756477]\n"
     ]
    }
   ],
   "source": [
    "# Exponential function\n",
    "exponential_values = []\n",
    "for j in simple_layer: # e**n (e power n)\n",
    "    exponential_values.append(exponential_math**j)\n",
    "\n",
    "print(exponential_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22.19795128 73.6997937   0.09071795]\n",
      " [ 0.13533528  0.36787944 90.0171313 ]\n",
      " [ 0.16529889  0.40656966  1.10517092]]\n"
     ]
    }
   ],
   "source": [
    "# power of numpy ... No more need of math.e (export math)\n",
    "expo_values = np.exp(layer_outputs)\n",
    "print(expo_values) # no more negatives to deal with (without losing them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09856589040931818, 0.2424329707047139, 0.6590011388859679]\n",
      "base values: 1.6770394660378334\n"
     ]
    }
   ],
   "source": [
    "# Normalization ... for example -1.8/(-1.8 + -0.9 + 0.1)\n",
    "# this keeps the value between 0 and 1\n",
    "\n",
    "norm_base = sum(exponential_values)\n",
    "norm_values = []\n",
    "\n",
    "for k in exponential_values:\n",
    "    norm_values.append(k/norm_base)\n",
    "\n",
    "print(norm_values)\n",
    "print(\"base values:\", norm_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[95.98846293]\n",
      " [90.52034602]\n",
      " [ 1.67703947]]\n",
      "[[2.31256451e-01 7.67798457e-01 9.45092259e-04]\n",
      " [1.49508137e-03 4.06405253e-03 9.94440866e-01]\n",
      " [9.85658904e-02 2.42432971e-01 6.59001139e-01]]\n"
     ]
    }
   ],
   "source": [
    "norm_np_base = np.sum(expo_values, axis=1, keepdims=True) # axis == 1(horizontal)\n",
    "print(norm_np_base) # keepdims == keeps dimensions same\n",
    "\n",
    "norm_np_values = expo_values/norm_np_base\n",
    "print(norm_np_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.24659696 0.81873075 0.00100779]\n",
      " [0.00150344 0.00408677 1.        ]\n",
      " [0.0018363  0.00451658 0.01227734]]\n"
     ]
    }
   ],
   "source": [
    "# Exponential function also has an issue\n",
    "# it can't take big values like 1000 \n",
    "# sol: subtract all values with biggest value and make them negative\n",
    "\n",
    "new_expo_values = np.exp(layer_outputs - np.max(layer_outputs))\n",
    "print(new_expo_values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0663355 ]\n",
      " [1.00559021]\n",
      " [0.01863023]]\n",
      "[[2.31256451e-01 7.67798457e-01 9.45092259e-04]\n",
      " [1.49508137e-03 4.06405253e-03 9.94440866e-01]\n",
      " [9.85658904e-02 2.42432971e-01 6.59001139e-01]]\n"
     ]
    }
   ],
   "source": [
    "norm_np_base = np.sum(new_expo_values, axis=1, keepdims=True) \n",
    "print(norm_np_base) # keepdims == keeps dimensions same\n",
    "\n",
    "norm_np_values = new_expo_values/norm_np_base\n",
    "print(norm_np_values) # YES!! Same values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax_activation:\n",
    "    def forward(self, inputs):\n",
    "        expo_values = np.exp(inputs - np.max(inputs))\n",
    "        # norm_base = np.sum(expo_values, axis=1, keepdims=True)\n",
    "        proba = expo_values/np.sum(expo_values, axis=1, keepdims=True)\n",
    "        self.output = proba # probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use this in next part with our main code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
